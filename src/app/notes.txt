0.598806, with that really small dataset of just 2 images, 2GPUs, batch size of 10 images
0.457151 with 4 gpus and batch 20
training with just one label instead of two did not help, next improve train.xml dataset and try again

changing chip size to 200x500 did not help, but improved speed of learning by probably 20 times, lets try even less resolution
100x250 0.66 training result worse than 200x500, going back up
200x500 is sweet spot
going up with batch size overfitted the model, but that is probably because we have this really small test dataset



loss 2.04 with max object size at 0.8, max object size does not make difference in loss
trying lower learning rate 0.5, did not help

Increasing training rate without threshold to 2000 helped a lot, trying 2500=worse

actual best result
20.01.2018
step#: 7478  learning rate: 0.0005  average loss: 0.460558     steps without apparent progress: 177
Training results:        1 0.833333 0.83333
