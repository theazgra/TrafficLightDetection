0.598806, with that really small dataset of just 2 images, 2GPUs, batch size of 10 images
0.457151 with 4 gpus and batch 20
training with just one label instead of two did not help, next improve train.xml dataset and try again

changing chip size to 200x500 did not help, but improved speed of learning by probably 20 times, lets try even less resolution
100x250 0.66 training result worse than 200x500, going back up
200x500 is sweet spot
going up with batch size overfitted the model, but that is probably because we have this really small test dataset



loss 2.04 with max object size at 0.8, max object size does not make difference in loss
trying lower learning rate 0.5, did not help

Increasing training rate without threshold to 2000 helped a lot, trying 2500=worse

actual best result
20.01.2018
step#: 7478  learning rate: 0.0005  average loss: 0.460558     steps without apparent progress: 177
Training results:        1 0.833333 0.83333


orange(label y[also must be renamed to o as orange]) is probably labeled wrong, 
it is labeled like one color (middle) or like two colors (top and middle one).
It would be probably best to go through dataset and clean this.

What is better orange label for 2 situations or 2 different labels??
Also re-label dataset, dont include white borders on TL, side TL does not have them!!!

dataset train2.xml
====================================
step#: 21199  learning rate: 0.0001  average loss: 0.72625      steps without apparent progress: 1796
Saved state to TL_SYNC_FILE
Training is completed.
Training results:  0.94382 0.743363 0.734304 



